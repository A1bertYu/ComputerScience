#INTRODUCTION TO ALGORITHM
second edition 潘金贵译 机械工业出版社
##第2章 算法入门
###Insertion sort
伪码  

	INSERTION_SORT(A)
	for j = 2 to n
		key = A[j]
		i = j-1
		while(A[i] > key && i > 0)
			A[i+1] = A[i]
			--i
		A[i+1] = A[j]

循环不变式（loop invariant）  
三个步骤，分别对应循环前，循环中和循环后：  
（1）初始化：先证明第一轮迭代之前，循环不变式是成立的；  
（2）保持：每一轮循环都能使循环不变式保持成立。  
（3）终止：循环结束后，证明循环不变式成立。  
上述所谓的循环不变式，是指与循环有关的某个变量（如已排序数组），在每次循环后，性质保持不变。例如，对于插入排序，每一次循环后产生的已排序的子数组，其称循环不变式。  
循环不变式与数学归纳法是很相似的，前两步一样，第三步前者是到循环结束（有限），后者则是归纳到无穷（无限）。  
插入排序是增量（incremental）方法，即在已排序的子数组中，将后续元素插入，形成最后的结果。
插入排序是**原地排序**，即在排序输入数组时，只有常数个元素被存放到数组以外的空间中去。
###算法分析
>一般来说，算法所需时间是与输入规模同步增长的，因而常常将一个程序的运行时间表示为其输入的函数。这就要求对术语“运行时间”和“输入规模”更加仔细地加以定义。（P13）

对于要分析的程序（即算法具体的实现，可以是伪代码），假定其每条语句运行时间为常数（每条语句对应不同常数），并计算出其执行次数（与输入有关），再对所有语句的用时求和，便可进行复杂度分析。算法性能通常与输入有关，且一般是用最坏情况进行衡量。
###算法设计
* 分治法  
将原问题划分成n个规模较小而结构与原问题相似的子问题；递归地解决这些子问题，然后再合并结果，就得到原问题的解。  
分治法在每一层递归中，都包括三个步骤：分解（Divide），解决（Conquer）和合并（Combine）。分解是将原问题分解成一系列子问题；解决是递归地解决各子问题（若子问题足够小，直接求解）；合并是指将子问题的解进行合并而得到原问题的解。
* Merge sort  

		MERGE(A, p, q, r)
		n1 = q - p + 1
		n2 = r - q
		create arrays L[1,...,n1+1] and R[1,...,n2+1]
		for i = 1 to n1
			L[i] = A[p+i-1]
		for j = 1 to n2
			R[j] = A[q+j]
		L[n1+1] = +inf
		R[n2+1] = +inf
		i = 1, j = 1
		for k = p to r
			if L[i] <= R[j]
				A[k] = L[i++]
			else
				A[k] = R[j++]
对于上述合并，有两点值得注意：（1）**哨兵元素的使用**，这样避免尾部的检测（若不使用，需要比较i或者j是否到达了n1或者n2，这样每一次循环多了两次比较）；（2）循环的设计，设计为一次循环就完成A数组的求取，这种情况下其实不用担心L或者R越界的。（1）是（2）的基础，有了（1）之后才能设计（2）的循环。  
有一种容易想到的解法是用while循环，来确保L，R不越界，在循环体内比较L和R的元素之后再赋值给数组，退出循环之后再将没有遍历完的数组赋值给数组A。该解法其实在每次循环都检测了尾部，多做了一次比较工作（L和R是否越界n1和n2，而此处所列解法只需判断是否越界r），因此不及此处所列解法。	

		Merge sort
		MERGE_SORT(A, p, r)
			if p < r
				q = floor((p + r)/2)
				MERGE_SORT(A, p, q)
				MERGE_SORT(A, q+1, r)
				MERGE(A, p, q, r)	
**理解递归**：对于合并排序中的递归，可以这么理解，递归调用时，最开始是直达底层，底层的整个函数处理完后（如本例中的两次合并排序递归再合并），再返回至上一层的递归调用处，沿着该处继续处理这一层的语句，若在这一层的后续中又遇递归，则又是直达底层，继而依前述流程执行。（可通过观察图2-4的执行过程进行体会学习）有些情况下我们会遇到**尾递归**，尾递归可以理解为一系列的代码展开，只是到达最底层后再逐层上浮。


* 分治法分析  
	根据分治法的特点，其执行时间可以用递归式进行表示**T(n) = aT(n/b) + D(n) + C(n)**, 其中D(n)和C(n)分别表示分解该问题和合并子问题的解的时间，而且在n小于某个值时，T(n)是常数。对比数列的**通项公式**，两者有点类似，可比较学习。
##第3章 函数的增长
渐进记号，定义的是一个集合，此外，其不等式成立的条件类似于证明数量极限时，只有对n > N时才成立。另外，算法性能的函数定义域是自然数集，且为正函数。

##第4章 递归式
在第2章分治法分析中提到了类似与数列通项公式的递归式。该递归式又可以写做：  
>T(n) = aT(n/b) + f(n) 其中，a >= 1, b > 1, f(n)是已知函数。b > 1体现了递归解决的方式是将大问题划归为小问题，这样才能求解，总不能越分越大吧！

关于递归式的解法，本章介绍了代换法、递归树和主定理两种方法。  

* 代换法  
	步骤，先猜测（需要经验甚至创造性），再用数学归纳法去证明。  
	证明时，最后推到出的表达式的结果一定要与猜测的相同，比如，假设T(n) <= cn,那么最后导出的是T(n)<=cn+1，这也是不满足要求的。当然，这样缩放式证明是可以的：T(n) <= cn-1 < cn  
	需要注意的地方是，要避免陷阱，比如T(n) <= cn，证出T(n) <= cn + n并没有得证，因为假设与最后导出的表达式要一致。  
	有些情况也可以使用变量替换法，求出结果后再替换回去。

* 递归树  
	画出一个递归树是一种得到好猜测的直接方法。递归的层数是最先使通项中T(n/b)中的自变量为1的层数（一般在T(c)递归到达底部，c为常数，假定为1与假定为其它常数也只差一个常数项，不影响分析），层数k满足等式power(b, k) = n。观察图4-1，每一层按等比为a进行裂变，直至到最后一层，因此，图4-1最后一层的项数为n*log(b,a)（注意，power(a, log(b,n)) = power(n, log(b, a))，参考3.15式）。

* 主方法（master method）  
	从图4-1递归树中的各层执行时间来看，在最后一层，执行时间已经与f(n)无关，而与power(n, log(b, a))有关，但前面各层的执行时间则与f(n)有关，因此，主方法推导出了三种情况，即根据f(n)与power(n, log(b, a))的关系，T(n)有不同的渐进界。   
	第二种情况递归层数是lgn，所以有该系数  
	第三种情况有个附加条件，来历可以参考图4-3  
	多项式小于（polynomially smaller），差一个power(n, epsilon)因子，epsilon为某大于0的常数。
##第5章 概率分析和随机算法
###雇佣问题
均匀的随机排列，对于n个元素的数组，其有n!种出现的次序，若每一种次序出现的概率都相同，则称为均匀的随机排列。
###指示器随机变量
首先回顾一下概率论的一些基本定义。  
**随机试验**：（1）可以在相同的条件下重复进行；（2）每次试验的可能结果不止一个，并且能事先明确试验的所有可能结果；（3）进行一次试验之前不能确定哪一个结果会出现。  
**样本空间**：随机试验E的所有可能结果组成的集合称为E的样本空间，记为S。  
**样本点**：随机试验E的每个结果，称为样本点。  
**随机事件（简称事件）**：样本空间S的子集为随机试验E的随机事件。在每次试验中，当前仅当这一子集中的一个样本点出现时，称这一 **事件发生**。  
**基本事件**：由一个样本点组成的单点集，称为基本事件。  
**随机变量**：设随机变量的样本空间为S={e}（注意：是集合），X=X(e)是定义在样本空间S的单值函数。称X=X(e)为随机变量。例如，X是投掷3次硬币得到正面的次数，在这个试验（投掷硬币3次）中，样本空间S可以确定，X的值也能确定，X的定义域显然是属于S的。对于某随机变量，其值的概率之和为1.  
**指示器随机变量**，关于随机事件A的函数，A发生值为1，A不发生值为0.显然，指示器随机变量的期望等于事件A发生的概率。增加了一层间接性，在寻找问题解决方法时多了一种思路，特别是在分析重复随机试验中的情况时。  
**应用例子**：  
（1）统计抛掷一枚硬币时正面朝上的期望次数。每一次抛掷时也对应一个随机事件，因此可以将每次抛掷的正面朝上事件用指示器随机变量来包装，总次数（也是随机变量）就是各指示器随机变量之和。  
（2）雇佣问题。雇佣新的办公助理的次数，假设应聘者以随机顺序出现，前i个应聘者中的任何一个都等可能的是目前最具资格的，因此，应聘者i被雇佣的概率为1/i，因此，雇佣次数就是所有应聘者被雇佣的期望之和。
###随机算法
许多随机算法通过排列给定的输入数组来使输入随机化。  

* 随机排列数组  
	一种常用的方法是为数组的每个元素A[i]赋予一个随机的的优先级P[i]，然后根据优先级对数组进行排序。  

		PERMUTE_BY_SORTING(A)
		n = length(A)
		for i = 1 to n
			P[i] = Random(1,n^3)
		sort A, using P as sort keys
		return A
	上述伪代码假定所产生的优先级都是唯一的，即数组P元素值各异
	关于上述产生的排列，其属于均匀随机排列的证明，思路是对于任意序列，证明其出现的概率为1/n!，书中对一种特殊情况，即第一个元素最小，第i个元素第i小，进而可以证明。其实，可以这样做更一般的假设，即第i个元素第j小，此时，依旧即该事件为Xi，进而可以得到同样的概率求解式子，只是在进行条件分解时，先分解第1小的，其次再是第2小的，以此类推，这样可以得到与书中特例的相同解1/n!。

		RANDOMIZE_IN_PLACE(A)
		n = length(A)
		for i = 1 to n
			swap(A[i], Random(i, n))
	<font color='red'>证明还有待添加</font>
###概率分析和指示器随机变量的进一步使用
* 生日悖论  
	求房间里的最少人数，使两个人有相同生日的机会超过50%。  
	书中给出了两种解法，都很有参考价值。对于第一种直接使用概率分析的解法，书中是从反面进行了求解，即所有人生日都不相同的概率，并且在通过通项的形式来进行推导，这种角度很值得学习。第二种使用了指示型随机变量，如何合理的设定指示型随机变量也是很有技巧的，书中是将两人生日相同的事件包装成了指示型随机变量（此时变量的意义是生日相同的对数（pair，不是log）），在假设所有人生日都独立，且所有人生日均匀分布在全年各天的情况下，求出两人生日相同的概率为1/n，进而可以求出指示型随机变量的期望，最后求出了生日相同的对数（pair，不是log）的期望，虽然没有按题意解答问题（50%概率），但也看到了另一种思路。
	<font color='blue'>关于该问题的一些思考：第一种解法显然是将生日组合划分成了两块，第一块是生日各不相同，第二块是生日不是各不相同（即两个人或者以上有相同的生日，这也是所要解决的问题），解法一是从反面解决该问题的。第二种解法，最开始以为是与第一种解法类似，只是从正面直接去计算概率（第二块的概率），且以为只考虑了两两相同的部分，而两两相同的部分因为重叠原因，也包括了三个相同四个相同...全部相同，但这样还是多算了，毕竟三个相同等也只是多个两两相同组成的。后来仔细思考了一番，发现理解错了，指示型随机变量虽然是指示的两人生日相同的事件，且其期望是两人生日相同的概率，**但多个指示型随机变量之和并不是概率**，而是两人生日相同这种事件发生的数量，这点一定要理解清楚，若是概率，人数很多时，岂不超过了1？此外，解法二只是求解了这种事件能发生（期望大于1）的人数，而未考虑大于50%的题意要求</font>
* 球与盒子  
	<font color='red'>本小点以及本章后续以后再补充。</font>
##第6章 堆排序
代码紧凑的算法，其运行时间所隐含的常数因子就很小。堆排序综合了插入排序（原地排序，in place）和合并排序（运行时间）的优点。
>堆排序还引入另一种算法设计技术：利用某种数据结构（在此算法中为“堆”）来管理算法执行中的信息。（P73）
###堆
二叉堆数据结构是一种数组对象，它可以被视为一颗完全二叉树。表示堆的数组A具有两个属性：length(A)表示数组中元素的个数，而heap-size(A)表示存放在A中堆的元素个数，而且heap-size(A)小于等于length(A)，A[1...heap-size(A)]中的元素为堆中的元素，其余元素则不是堆中的元素。树的根为A[1]  
		
	PARENT(i)
		return floor(i/2)
	LEFT(i)
		return 2*i
	RIGHT(i)
		return 2*i + 1
二叉堆分大根堆和小根堆，在大根堆中，节点i需要满足的性质是A[PARENT(i)] >= A[i]；小根堆则是A[PARENT(i)] <= A[i]。
###保持堆的性质
	MAX_HEAPIFY(A, i)
	l = LEFT(i)
	r = RIGHT(i)
	largest = i
	if l <= heap-size(A) && A[i] < A[l]
		largest = l
	if r <= heap-size(A) && A[largest] < A[r]
		largest = r
	if largest != i
		swap (A[i], A[largest])
		MAX_HEAPIFY(A, largest)
上述代码通过数组中元素位置，来使得最大堆性质得以保持。关于上述代码的时间分析，元素i肯定与下标比自己大的元素（子女元素）交换（若需要交换的话），随着下标的增大，渐渐的会使得递归返回。那相邻的递归之间运行时间有什么关系呢？通过代码，可以列出通项T(n) = T(n/b) + C，那最坏的情况肯定是b最小的情况，也就是问题规模减少的最少，也就是子树结点数量最大可能的占比，借助图6-2，若是对A[1]进行调整（根结点），且若递归的是左子树（并且图6-2中A[5]要有右子女），此时左子树占比最大6/11，小于2/3，之后的递归不会比6/11大，因此缩放得到书中的递归式。
###建堆	 
	BUILD_MAX_HEAP
	heap-size(A) = length(A)
	for i = floor(length(A) / 2) downto 1
		MAX_HEAPIFY(A, i)
在建堆的过程中，叶结点是不需要管的，因为其没有子结点，无需考虑堆的性质（父结点与子结点关系）。此外，建堆的过程是从下到上来进行建立。  
###堆排序	
	HEAP_SORT
	BUILD_MAX_HEAP
	for i = length(A) to 2
		swap(A[1], A[i])
		heap_size(A) = heap_size(A) - 1
		MAX_HEAPIFY(A, 1)
###优先级队列
最大优先级队列和最小优先级队列。下面对前者进行介绍，包括四个操作：MAXIMUM(S)，EXTRACT_MAX(S)，INCREASE_KEY(S, x, k)和INSERT(S,x)  

* 获取最大值  

		MAXIMUM(A)
		return A[1]
* 去除最大元素	
		
		EXTRACT_MAX(A)
		max = A[1]
		swap(A[1], A[heap-size(A)])
		heap-size(A) = heap-size(A) - 1
		MAX_HEAPIFY(A, 1)
		return max
* 将堆中某元素的值更换为更大的值

		INCREASE_KEY(A, i, key)
		if A[i] > key
			error, key must bigger than the current key
		while (i > 1 && A[Parent(i)] < A[i])
			swap(A[Parent(i)], A[i])
			i = Parent(i)

* 添加元素

		INSERT(A, key)
		heap_size(A) = heap_size(A) + 1
		A[heap_size(A)] = -inf
		INCREASE_KEY(A, heap_size(A), key)
##第7章 快速排序
快排最坏情况运行时间是O(n^2)，期望运行时间是O(nlog(n))，且常数因子较小，通常在实践中优先采用快排。 
###快速排序的描述
		QUICK_SORT(A, p, r)
		if r > p
			q = PARTITION(A, p, r)
			QUICK_SORT(A, p, q-1)
			QUICK_SORT(A, q+1, r)
上述排序算法最重要的是PARTITION(A)的实现，书中的实现如下：  

		PARTITION(A, p, r)
		x = A[r]
		i = p
		for j = p to r-1
			if(A[j] <= x)
				swap(A[i++], A[j])
		swap(A[i], A[r])
		return i
上述PARTITION代码中，主元（pivot element）的选择很为重要。
###快速排序的性能
>快速排序的运行时间与划分是否对称有关，而后者又与选择了哪一个元素来进行划分有关。（P88）

* 最坏情况划分  
	每次划分后，元素全部在主元的某一边，此种情况为最坏情况（最不对称），T(n)=T(n-1) + T(0) + C（可看做一等差数列），其运行时间为O(n^2)。
* 最佳情况划分  
	每次划分后，主元两侧元素一样多（即两边对称），T(n) <= 2T(n/2)+C，其运行时间为O(nlog(n))
* 平衡的划分  
	书中举例了常数比例的划分，9:1和99:1两种情况，运行时间也是O(nlog(n))。  

随机化版本的快排代码如下，  

	RANDOMIZED_QUICKSORT(A, p, r)
	if p < r
		q = RANDOMIZED_PARTITION(A, p, r)
		RANDOMIZED_QUICKSORT(A, p, q-1)
		RANDOMIZED_QUICKSORT(A, q+1, r)

其中，主元的选择是随机在数组中选择一个元素，之后再按之前的PARTITION进行划分，  
		
	RANDOMIZED_PARTITION(A, p, r)
	i = RANDOM(p, r)
	swap(A[i], A[r])
	return PARTITION(A, p, r)

###快速排序的分析
* 最坏情况分析  
	通过放缩法，列出式子7.1，即在每一轮中，找出用时最大的划分，同时猜测T(n) <= cn^2并得证，而我们可以找出运行时间就是n^2的特例，因此可以得到更渐进的运行时间。  
* 期望的运行时间  
	首先书中引理7.1证明了快速排序的运行时间为O(n+X)，其中，n为调用PARTITION的次数，而X则是PARTITION进行比较操作的总次数。关于比较的次数，两个元素之间最多只进行一次比较，且划分之后，主元两侧的元素是不会进行比较的，比如{1,3,2} 4 {7, 9}，4为主元，4前半部分的元素和后半部分的元素不会再进行比较，前半部分只会与前半部分的进行比较，而后半部分只会与后半部分进行比较。书中引入了指数随机变量，用以包装两个元素需要比较的事件。此外，书中对数组A中的元素进行了重命名，即zi表示第i个最小的元素，**注意，这并不是说A中的数组元素依次为z1,z2,z3,...**。同时，书中又定义了集合Zij = {zi,...,zj}。对于任选的两个元素Zi和Zj，其需要比较的概率是在集合Zij中选择zi或者zj做主元的概率，对于其它情况，两个元素不需要进行比较。**在这里，突然跟Zij扯上关系，刚开始感觉有点比较特例化，若zi和zj之间的元素与Zij不一致如何？**后来仔细一想，并且结合7.2式，第一个求和符号确定了i之后，后续的j是从i+1直至n，也就是说zi与zj之间是有距离j-i-1的，这段距离之间填充的元素数量是j-i-1，至于填充的这些元素是否大于zi并小于zj，无关紧要，且将填充元素假设成集合Zij的，产生的结果也是一样的。只要求得Zi与Zj在所有可能的长度中被选做主元的概率，即对于每一个i，分别求长度2,...,n-i+1对应的j（长度计算时包括zi,zj），并进而求的这些长度对应的选中概率，就可求的zi与zj的比较概率。

##第8章 线性时间排序
###排序算法时间的下界
>**比较排序** 可以被抽象成**决策树** 。一颗决策树是一颗满二叉树，表示某排序算法作用于给定输入所做的**所有比较**。（P97）  
>要使排序算法能正确地工作，其必要条件是，n个元素的n!种排列中的每一种都要作为决策树的一个叶子而出现。（P97）

高为h的决策树，叶子数目不会多于2^h，因此n! < 2^h，解得h >= nlog(n)，这表示**比较排序**（最坏情况下）需要比较h次，也就是下界为O(nlog(n)).
###计数排序
计数排序假设输入的n个元素都是介于0到k的整数，当k=O(n)时，计数排序的运行时间为线性O(n+k)。

	COUNTING_SORT(A, B, k)
	for i = 0 to k
		C[i] = 0
	for j = 1 to length(A)
		C[A[j]] = C[A[j]] + 1
	for i = 1 to k
		C[i] = C[i] + C[i-1]
	for j = length(A) to 1
		B[C[A[j]]] = A[j]
		C[A[j]] = C[A[j]] - 1
上述排序的原理，主要是用数组C先记录A中各元素出现的次数，再将次数累积，进而可以求得A中个元素的**位置信息并存入数组C**，且C中各元素记录的是A中对应元素的最大位置，比如C[3] = 5，则表明A中值为3的元素最大的位置是5，若有两个值为3的元素，则位置4也是值为3的元素。因此，在最后一个for循环中，从length(A)开始，是为了让最大位置放置最后出现的元素（对于相同值的元素来说），这样以保证稳定性，即输出数组中的相对次序与它们在输入数组中的次序相同（这一般是对具有相同值的元素来说）。
###基数排序
基数排序首先按最低有效位数字进行排序，对于d位数字，仅需d遍就可以完成排序，若是按最高有效位开始排序，则需要记录很多中间数据，反而不便。

	RADIX_SORT(A, d)
	for i = 1 to d
		do use a stable sort to sort array A on digit i,
上面代码中1表示最低位，d表示最高位。  
引理8.3中，对每一位数字采用计数排序，用时n+k，共d位，则为d(n+k)  
引理8.4中，中文版翻译是不准的，其翻译的是n位数（而英文版是Given n b-bit numbers，而不是Given n b-digit numbers），准确翻译应该是n位二进制数，若不这样指出，引理是不成立的，因为若n位数的进制就大于2^r，r位二进制就根本表示不了，怎么办？此外，该引理讨论了b, r, n的关系对算法的影响。
###桶排序
>当桶排序（bucket sort）的输入符合均匀分布时，即可以以线性时间运行。与计数排序类似，桶排序也对输入作了某种假设，因而运行得很快。具体来说，计数排序假设输入是由一个小范围内的整数构成，而桶排序则假设输入由一个随机过程产生，该过程将元素均匀地分布在区间[0,1)上。（P102）

	BUCKET_SORT(A)
	n = length(A)
	for i = 1 to n
		insert A[i] into B[floor(n*A[i])]
	for i = 0 to n-1
		sort list B[i] with insertion sort
	concatenate the list B[0], ..., B[n-1] together in order
桶排序的复杂度为O(n)，建立在如下假设（或关系），输入均匀分布；或者各个桶尺寸的平方和与总的输入元素数量呈线性关系。
##第9章 中位数和顺序统计学
在一个由n个元素组成的集合中，第i个顺序统计量（order statistic）是该集合中第i小的元素。一个中位数（median）是它所在集合的“中点元素”，不考虑奇偶，中位数可表示为下中位数和上中位数，分别出现在floor[(n+1)/2]和ceiling[(n+1)/2]处（从1开始）。
###最大值和最小值
最大值和最小值的查找需要比较n-1次，是O(n)；若同时查找最大值和最小值，至多floor(3n/2)次比较。将输入数组两两分组，先两者比较，之后较大者跟最大值比较，较小者跟最小值比较，这样一对元素只需比较3次。
###以期望线性时间做选择
	RANDOMIZE_SELECTION(A, p, r, i)
	if p == r
		return p
	q = RANDOMIZED_PARTITION(A, q, r)
	k = q - p + 1
	if i == k
		return A[q]
	else i < k
		return RANDOMIZE_SELECTION(A, p, q-1, i)
	else
		return RANDOMIZE_SELECTION(A, q+1, r, i-k)
此算法的思想与快速排序相同，也是对输入数组进行递归划分，只不过快速排序会递归地处理两边，而此算法只会处理一边，其期望运行时间为O(n)。但最坏运行时间为O(n^2)，每次划分时只有一边有元素。  
在上述算法中，RANDOMIZED_PARTITION以等概率的返回任一元素作为主元，即以1/n的概率返回q值，也就是k取1到n的值的概率是1/n，这样，左右两边的数组长度分别为k-1和n-k，对于不同k值(1 to n)，对应不同长度，这些不同长度的概率是1/n。因此，通过简单的缩放，假定每次都处理两边数组中的较长的那一边，可以按书中所列递归式进行处理。注意，书中利用了指示型随机变量进行分析，因为指示型随机变量用于表示事件发生或没发生，可以认为是一个bool型变量，对不同k值，需要进行叠加，这样才考虑了所有的可能性。若不用随机性变量，每一种可能性1/n，可以直接得到后面的求期望的表达式。
###最坏情况线性时间的选择
该算法最坏情况也是O(n),方法依旧是对输入数组进行递归划分，但要保证每次划分是好的划分。
之所以能够保证每次都是好的划分，是因为选择了恰当的主元。该算法的在每次递归中，都是选择中位数做为主元，这样，每次划分都是最好的划分。  
具体步骤如下：  
（1）将输入数组划分为floor(n/5)组，即每个组5个元素，当n不是5的倍数时，剩下的组成一个组。  
（2）在每个组中，使用插入排序寻找中位数。  
（3）将寻找到的中位数序列，再次进行划分，直至找到整个数组的中位数x  
（4）将x作为主元，对数组进行划分，假设x为第k个元素，则左边有k-1个元素，右边有n-k个元素。  
（5）若i=k，则返回x；若i<k，则递归调用在左边寻找第i个元素；若i>k，则递归调用在右边寻找第i-k个元素。  
**注意**，此算法运行时间的递归表达式中各项的意义需要清楚，文中也分别对各步骤的运行时间给出了表达式。需要注意的是，总的运行时间称为SELECT的时间，用T(n)表示，而T(n)又划分成了两个子问题，单纯的寻找中位数为T(n/5)，寻找第i个统计量缩放为T(7n/10 + 6)。每一层递归中，前者对应步骤3，而有需要借助步骤1和2（为O(n)）；对于后者，则对应步骤5，而需要步骤4（为O(n)）。最开始看感觉步骤3和步骤5不是同一个问题，其实步骤3也是找顺序统计量，为中位数，这样就可以看做一个问题，进而列出了书中通项。<font color='red'>实际程序中这两个应该不是同一类型吧，至少不能调用相同的递归程序，因为步骤(3)肯定要先找到中位数，以来用作主元。</font><font color='blue'>此外，步骤2的运行时间为O(n)，每一个小组的插入排序运行时间可以看做O(1)，总体来说是O(n)，可以与全局的插入排序做一下比较，分析下运行时间不同的原因</font>
##第10章 基本数据结构
###栈和队列
栈和队列都是动态集合。栈后进先出LIFO，队列则是先进先出FIFO。对空栈进行弹出操作，称为下溢；对栈满了再压入，则为上溢。

* 栈操作

		STACK_EMPTY(S)
		if top(S) == 0
		return true
		else
		return false
	上述代码为空栈检查。

		PUSH(S, x)
		top(S) = top(S) + 1
		S[top(S)] = x
	上述代码为入栈(未检查溢出)

		POP(S)
		if STACK_EMPTY(S)
			"error, underflow"
		else
			top(S) = top(S) - 1
			return S[top(S)+1]
	上述代码为出栈

* 队列操作  
	head(Q)为队列的第一个元素，tail(Q)指向新元素被插入的地方。head(Q)=tail(Q)表示队列为空，初始时head(Q)=tail(Q)=1  

		ENQUEUE(Q, x)
		Q[tail(Q)] = x
		if tail(Q) = length(Q)
			tali(Q) = 1
		else
			tail(Q) = tail(Q) + 1
	上述代码为入队(未检查溢出)

		DEQUEUE(Q)
		x = Q(head(Q))
		if head(Q) = length(Q)
			head(Q) = 1
		else
			head(Q) = head(Q) + 1
		return x
	上述代码为出队(未检查溢出)

###链表
>在链表这种数据结构中，各对象按线性顺序排序。链表与数组不同，数组的线性序是由**数组下标决定** 的，而链表中的顺序是由各对象中的**指针所决定**。(P121)

下面代码都是假定链表为双向的和无序的，链表每个元素包含一个关键字域和两个指针域。对于整个链表，还需要一个指针，用于指向链表当前的首部元素，并且在首部元素有变动时需要更新。  
**注意：以下代码都是对指针进行操作。此外，书中都是用[]，而此处用的()，因为()更像函数调用来返回指针。只有对哨兵元素，此处用了[]，以表示不是指针，而是包含了prev,next,key三个成员的元素（对象）**

	LIST_SEARCH(L, k)
	x = head(L)
	while x != NIL && key(x) != k
		x = next(x)
	return x
上述代码为链表的搜索操作，最坏情况运行时间为O(n)
	
	LIST_INSERT(L, x)
	next(x) = head(L)
	if head(L) != NIL
		prev(head(L)) = x
	head(L) = x;
	prev(x) = NIL;
上述代码为链表的插入操作（插入到首部）。

	LIST_DELETE(L, x)
	if prev(x) != NIL
		next(prev(x)) = next(x)
	else
		head(L) = next(x)
	if next(x) != NIL
		prev(next(x)) = prev(x)
哨兵（sentinel）是个哑（dummy）对象，可以简化边界条件，能使代码更加简洁，可以降低算法的常数因子。但哨兵的使用还是因情况而定，比如短链表，使用哨兵后会造成存储的浪费。  
对于双链表，引入哨兵元素nil[L]，其next指向表头，prev指向表尾，同时表尾的next和表头的prev指向nil[L]**此处用了中括号**，如此，双链表成了一个带哨兵的循环双向链表。上述三种操作的对应代码为：

	LIST_SERACH(L, k)
	x = next(nil(L))
	while x != nil(L) && key(x) != k
		x = next(x)
搜索代码

	LIST_INSERT(L, x)
	next(x) = next(nil(L))
	prev(x) = nil(L)
	prev(next(nil(L))) = x
	next(nil(L)) = x
插入到首部的代码

	LIST_DELETE(L, x)
	next(prev(x)) = next(x)
	prev(next(x)) = prev(x)
删除代码
	
###指针和对象的实现
使用数组和数组下标来构造对象和指针。  

* 对象的多重数组表示
	书中使用三维数组对双链表进行了实现，其中，prev中存储的上一个元素的索引，而next则存储下一个元素的索引，若是表头或表尾，则存储一个非有效索引。而第一个元素的索引，则需要一个变量专门保存，以记录链表表头位置。

* 对象的单数组表示
	类似于多重数组，比多重数组更为灵活，因为可以存放不同长度的对象（即数组元素所占用的内存大小可以不同），但是实际中考虑的大多数数据结构都是由同构元素组成（即占据数组长度相同）。

* 分配和释放对象  
	书中举了一个由多重数组表示的双链表，对于没有被使用的元素（自由元素），仅使用其next数组，由此构成一个单链表（自由表），该表的表头被置于全局变量free中。自由表示一个栈，下一个分配的对象是最近被释放的那个。一个自由链表可以被几个链表所共用。

		ALLOCATE_OBJECT()
		if free == NIL
			"error, out of space"
		else
			x = free
			free = next(x)
			return x
	上述代码为分配一个元素空间

		FREE_OBJECT(x)
		next[x] = free
		free = x
	上述代码为释放一个元素空间
###有根树的表示
* 二叉树  
	用域p, left和right来存放指向二叉树T中的父亲、左儿子和右儿子的指针。若P(x) = NIL，则x为根，若root(T) = NIL，则树为空。
* 分支数无限制的有根树  
	每个结点都包含一个父指针p、左孩子指针left_child和右兄弟指针right_sibling，如此，可以用二叉树来表示具有任意子女数的树。
* 树的其它表示  
	如第6章用数组加上下标的形式来表示基于完全二叉树的堆。第21章从叶结点向根结点方向遍历的树，只用了父指针，而没有子女指针。具体用法还需要根据问题来确定。

##第11章 散列表
散列表（hash table）是普通数组概念的推广。在散列表中，不是直接把关键字用作数组下标，而是根据关键字计算出数组下标。
###直接寻址表 Direct-address tables
适用范围，当关键字全域U比较小时，直接寻址是一种简单而有效的技术。需要注意的是，对于没有存放关键字的槽，必须要提供检查其为空的方法。
###散列表Hash tables  
当存储在字典中的关键字集合K比所有可能的关键字域U要小很多时，散列表所需的存储空间要比直接寻址表少很多。  
散列函数： h: U -> {0, 1, 2, ..., m-1}  
散列函数h必须是确定的，即某一给定输入k应始终产生相同的结果h(k)。  

* 通过链接法解决碰撞Collision resolution by chaining  
	在链接法中，把散列到同一槽中的所有元素都放在一个链表中。

		CHAINED_HASH_INSERT(T, x)
			insert x at the head of list T[h(key(x))]
此处的插入操作为O(1)，但是没有检查元素是否重复。

		CHAINED_HASH_SEARCH(T, k)
			search for an element with key k in list T[h(k)]
搜寻操作

		CHAINED_HASH_DELETE(T, x)
			delete x from list T[h(key(x))]
此处的删除操作是以元素x，而不是k，若是双链表，则由第10章“链表”一节可知，删除一个元素的时间是O(1)。若是单链表或者删除的是key而不是x，则需要进行搜寻，此时删除操作和搜寻操作的运行时间是相同的。

* 对用链接法散列的分析  
	装载因子(load factor)：能存放n个元素，具有m个槽的散列表T，装载因子alpha = n/m，alpha可以大于、等于或者小于1.  
	**简单一致散列**（simple uniform hashing）：假设任何元素散列到m个槽中的概率是相同的，且与其它已经被散列到槽中的元素无关（P135）。对比文中对于好的散列函数的描述，每个关键字都等可能地散列到m个槽位中的任何一个，并与其它关键字已经被散列到哪一个槽中无关（P137）。**好的散列函数才近似地满足简单一致散列**。在简单一致散列中，即使相同的关键字已被散列到槽h(k)中，该关键字再次被散列时，依然是等可能散列到槽h(k)中的。  
	**一致散列**：假设每个关键字的探查序列是(0,1,...,m-1)的m!种排列中的任一种的可能性是相同的，称之为一致散列。一致散列是将简单一致散列概念加以一般化，推广到散列函数的结果不止一个数，而是一个完整的探查序列的情形。<font color='blue'>注意，一致散列只是在开放寻址法中应用</font>  
	在简单一致散列假设下，用链接法解决碰撞的散列表，一次不成功查找的期望时间为Theta(1+alpha)。证明中，假定散列值计算和寻址槽的时间为O(1)。在简单一致散列的假设下，每个槽的平均长度为n/m = alpha，搜寻元素时找不到意味着将链表T[h(k)]遍历了一遍，而链表的平均长度为alpha，加上前面的O(1)，于是得到定理11.1  
	在简单一致散列假设下，用链接法解决碰撞的散列表，一次成功查找的期望时间为Theta(1+alpha)。该证明的思路为，对于被寻找元素x，在其所在的链表中，若排在x前面的元素数量为y，则查找到x的次数就为y+1（加1是比较x本身的时候）。对于表中的n个元素，求出寻找每一个元素所用的次数，再对这些次数求平均，即得到定理11.2.对于插入到表中的第i个元素xi，寻找其的次数为1+sum(Xij)，其中，j从i+1到n，这是因为新插入的元素都排在x的前面，即第i+1到n的元素，相对xi，都是新插入的，搜寻xi时，必须要经过它们，而经过它们的概率（即这些元素与xi处于相同的槽），是1/m，再利用指示型随机变量，便列出了书中的式子。

###散列函数 Hash Functions
* 将关键字解释为自然数  
	多数散列函数都假定关键字域为自然数集N={0,1,2,...}，若所给关键字不是自然数，则需要先映射到自然数。在设计散列函数时，最好考虑关键字的所有位的情况，即尽可能使所有位起作用。

* 除法散列法  
		
		h(k) = k mod m
	m不应为2的幂，不然只是对关键字k取最低p位（若m是2的p次幂），即只有最低p位起作用；m不应为2^p - 1，不然只是对k的各bit进行排列（<font color='red'>待理解</font>）。m值常常选择与2的整数次幂不太接近的质数。

* 乘法散列法  
	需要提供两个值，常数A（0<A<1）和槽数m，散列函数为  
		
		h(k) = floor(m* (kA mod 1))
	即用kA的小数部分乘以槽数m，得到散列值。如图11-4中所示，kA的小数部分，即ks相乘后（其积为r1*2^w + r0），再除以2^w，得到结果r0，之后再对2^p取余，相当于取r0中的p个最高位。
* 全域散列 Universal hashing  
	随机地选择散列函数，使之独立于要存储的关键字，这种方法称作全域散列。（注意，是先从一组散列函数中随机地选择一个散列函数，用该函数将n个关键字映射到m个槽中）
	>设H为有限的一组散列函数，它将给定的关键字域U映射到{0,1,...,m-1}中，这样的一个函数组称为是全域的。

* 定理11.3  
	从全域函数组中任选一个散列函数，用该函数将n个关键字散列到m个槽中。对于关键字k，其散列到链表中的期望长度与k是否在表中有关。（该结论看似有点绕，即k怎么可能不在表中呢？其实对于n个关键字，确实有可能就不包括关键字k）注意证明中关于指示型随机变量的应用。
* 推论11.4  
	注意包含O(m)个INSERT操作是已知条件。
* 设计一个全域散列函数类  
	<font color='red'>待完善，需要先看第31章</font>

###开放寻址法
在开放寻址法中，每个元素都存放在散列表中，即每个表项（table entry）要么包含一个元素，要么包含NIL。相比于链接法，因为不需要存储指针（链接法的每一个槽中存储的是链表指针，链表中才开始存储元素），从而节省空间，这样，就可以用同样的空间提供更多的槽，潜在效果就是减少碰撞，提高查找速度。这种方法，散列表可能会被填满，以致于不能插入新元素，这样装载因子alpha不会超过1  

* 插入元素  
		
		HASH_INSERT(T, k)
		i = 0
		repeat j = h(k, i)
			if T[j] == NIL
				T[j] = k
				return j
			else
				i = i + 1
		until i == m
		"error: hash table overflow"
插入一个元素时，如上述代码，连续地探查散列表的各项，直到找到一个空槽，上述代码的探查序列为h(k,0), h(k,1),...,h(k,m-1)。注意，对每一个关键字k，探查序列h(k, 0), h(k, 1), ..., h(k, m-1)必须是0,1,...,m-1的一个排列。
* 查找元素  
	
		HASH_SEARCH(T, k)
		i = 0
		repeat j = h(k, i)
			if T[j] == k
				return j
			else
				i = i + 1
		until T[j] == NIL or i == m
		return NIL
查找元素时，其探查序列与插入时是一致的。若搜寻到空槽（NIL），则表明表中没有该关键字k，当然，此处假定了之前插入的关键字不会被删除。
* 删除元素  
	删除元素比较复杂，若将删除后的槽置为NIL，则前述查找元素算法不能用，有一种方法是将被删除位置标记为DELETE，插入和查找算法基本无需调整。但删除元素的应用中，查找的时间就不依赖于装载因子alpha（比如，删除元素数量跟当前元素数量一致时，那搜寻的速率就是O(n)了），这时一般会使用链接法。
* 线性探查  
	根据散列函数算出h(k)，对槽(h(k)+i) mod m进行搜寻,其中i从0到m-1。首先i取0，若槽(h(k)+0) mod m被占用，接着i取1，直至m-1.  
	该方法的优点是实现容易，但其存在一次群集（primary clustering）问题，随着逐步插入元素，**连续占用**的槽不断增加，平均查找时间也不断增加。若一个空槽前面是i个被连续占用的槽，则该空槽下次被占用的概率为(i+1)/m，因为探测位置若是这i个位置之一，则最后必到该空槽，另外，探测位置若就是该空槽，最后也是该空槽，这样又导致了连续占用的槽的增长，这就是群集现象容易出现的原因。  
	此外，该探查序列只有m中，因为选定最开始的探查位置后，就确定了该探查序列，而只能选择m个最开始探查的位置。如此，便不能实现一致散列的假设。
* 二次探查  
	类似于线性探查，只是对于h(k)的偏移采用关于i的二次函数实现。若两个位置的初始探查位置相同，其探查序列也是相同的，这会导致二次集群（secondary clustering）。如同线性探查一样，初始探查决定整个序列，故只有m种不同的探查序列被用到。  
* 双重散列  
	使用了两个散列函数h1(k)和h2(k)，初始探查位置为T[h1(k) mod m]，后续探查加上偏移量h2(k) mod m。其中，h2(k)要与m互质，以便查找整个散列表。每一对可能的(h1(k), h2(k))都产生了一个不同的探查序列，因此共m^2种。
* 对开放寻址散列的分析  
	>在一致散列这种理想的方法中，用于插入或查找每一个关键字k的探查序列（h(k, 0), h(k, 1), ...., h(k, m)）为(0,1,...,m-1)的任一种排列的可能性是相同的。当然，每一个给定的关键字有唯一固定的探查序列。我们这里想说的是，考虑到关键字空间上概率分布及散列函数施于这些关键字上的操作，每一种探查序列都是等可能的。（P144）

	关于上述这段话的理解，要结合一致散列的定义，散列函数的结果不止一个，而是一个序列，我们暂且称之为散列函数序列。散列函数序列能够产生m!种的排列，对于具体的（给定的）关键字，其探查序列是确定的，但该序列与其它序列在散列函数序列产生的所有序列中占比是相同的。（注，所有序列肯定为m!种，但数量可能是整数倍的m!个）
* 定理11.6  
	证明的思路是求出进行i次探查后才发现不成功的概率，考虑所有情况（即i的取值从1到+inf），求出总和。关于一致散列假设在该证明中的作用，是这样的，一致散列下，查找的探查序列也是等可能的为(0,1,...,m-1)全排中的一种，因此，Pr(A1) = n/m(查找序列的第一个位置，对于所有槽都是等可能的)，接着的(n-j+1)/(m-j+1)也可类推。  
	推论11.7显然可证。
* 定理11.8  
	注意该定理相对于11.6，多了一个假设条件，即表中的每个关键字被查找的可能性是相同的。另外，要注意查找关键字k的探查序列与插入关键字k的探查序列是相同的，这样可以使用推论11.7。
* 一些思考  
	散列函数的定义域为自然数集（即关键字为自然数），关键字有多少个，范围及分布如何，这个与具体问题有关，但散列函数的作用就是将分散的或太大的关键字映射成连续的较小的散列值，这样方便使用数组存储和查找。散列函数并不一定是一对一的，很多情况下可能是多对一的，这就会发生碰撞，就有了探查的问题，也就有了探查序列的生成问题，进而导出了一致散列的定义。
###完全散列
散列技术，有着出色的期望性能，但当关键字为静态的（确定了，就不会通过期望值反映），散列技术可能获得最坏的性能。  
如果某一种散列技术在进行查找时，其最坏情况内存访问次数为O(1)的话（<font color='blue'>此处其实还是为期望次数，只是因为用了全域散列，不会在关键字为静态时，可能遭遇最坏性能的情况（这种情况是确定的，在非全域散列，且散列函数也确定了的话</font>），则称其为完全散列（perfect hashing）。  
完全散列的设计思路：采用两级散列法，每一级都采用全域散列，第一级将n个关键字映射到m个槽中，每一个槽又放置着一个小的散列表，第二级也就是将第i个槽中的ni个关键字散列到大小为ni^2的散列表中。  

* 定理11.9  
	证明了在全域散列的情况下，n个关键字散列到n^2个槽中，发生碰撞的概率小于0.5.定理的后面说到了对于大小为mj=nj^2的二次散列表，查找散列到该表中的nj个关键字，可以做到无碰撞的常量时间。这是因为碰撞的期望次数E[X]<0.5	
* 定理11.10  
	证明在全域散列的情况下，将n个关键字散列到m（m=n）个槽中，所有槽的长度平方之和的期望小于2n。
* 推论11.11  
	根据定理11.10，证明存在完全散列方案，使得二次散列表所需的存储总量的期望小于2n  
* 推论11.12  
	给出了二次散列表的存储总量为O(n)的大致概率。
##第12章 二叉查找树
###二叉查找树
二叉查找树中结点的关键字**性质**：结点x的左子树中的所有结点，其关键字小于等于x的关键字，而右子树中的所有结点则大于等于x的关键字。

* 中序遍历  
	对于每一个结点，其关键字的输出介于其左子树和右子树的关键字之间。  
* 前序遍历  
	对于每一个结点，其关键字的输出在其左子树和右子树的关键字之前。
* 后序遍历  
	对于每一个结点，其关键字的输出在其左子树和右子树的关键字之后。
###查询二叉查找树
在以x为根结点的子树中，根据键值k查找元素：递归法和迭代法  
递归法

	TREE_SEARCH(x, k)
	if x == NIL or key(x) == k
		return x
	if key(x) > k
		TREE_SEARCH(left(x), k)
	else
		TREE_SEARCH(right(x), k)
迭代法

	ITERATIVE_TREE_SEARCH(x, k)
	while x != NIL && key(x) != k
		if key(x) > k
			x = left(x)
		else
			x = right(x)
	return x 

* 最小关键字  
	
		TREE_MINIUM(x)
		while left(x) != NIL
			x = left(x)
		return x
* 最大关键字  
		
		TREE_MAXIMUM(x)
		while right(x) != NIL
			x = right(x)
		return x
* 前趋

		TREE_PREDECESSOR(x)
		if left(x) != NIL
			return TREE_MAXIMUM(left(x))
		y = p(x)
		while y != NIL && x == left(y)
			x = y;
			y = p(x);
		return y

* 后继  
	
		TREE_SUCCESSOR(x)
		if right(x) != NIL
			return TREE_MINIUM(right(x))
		y = p(x)
		while y != NIL && x == right(p)
			x = y
			y = p(x)
		return y
后继的运行时间为O(h)，h为树的高度，查找前驱元素是要么向下查找，要么向上查找。向下查右子树的最小结点。向上则查是否为某个结点的左子树。例如，对于最大关键字结点，其会一直插到根节点为止，返回NIL  
* 插入  

		TREE_INSERT(T, z)
		y = NIL
		x = root(T)
		while x != NIL
			y = p(x)
			if key(x) < key(z)
				x = right(x)
			else
				x = left(x)			
		p(z) = y
		if y == NIL
			root(T) = z
		else if key(z) < key(y)
			left(y) = z
		else
			right(y) = z
从上面代码，我们可以看出，新插入的元素必定在叶结点的位置，这是因为，二叉查找树的性质是左子树所有键值不大于本结点键值，而右子树所有键值不小于本结点键值，因此，最安全的插入方法就是将新结点放置叶结点的位置。因为放中间的话，很难以判断新结点是否满足二叉查找树的性质。  
若二叉查找树中的某个结点有两个子女，则其后继没有左子女，其前趋没有右子女。  
* 删除
	
		TREE_DELETE(T, z)
		if left(z) == NIL || right(z) == NIL
			y = z
		else
			y = TREE_SUCCESSOR(z)

		if left(y) != NIL
			x = left(y)
		else
			x = right(y)

		if x != NIL
			p(x) = p(y)

		if p(y) == NIL
			root(T) = x
		else if y == left(p(y))
			left(p(y)) = x
		else
			right(p(y)) = x
		if y != z
			copy y's satellite data into z
		return y
对于删除，主要思想是**删除只有0个或1个子女的结点**，因为这样调整最少，只需将子女移到父结点所在位置即可，而子女不管是否再有子女节点（孙结点），因子女还存在，这些结点是不需要调整的。试想，若删除的结点包含两个子女，若将左子女或者右子女移到父结点位置，此时就可能发生父结点有3个子女结点的情况（原来的父结点还剩一个，而新父结点原来就可能有两个子女），那将是非常难以调整。删除中利用了一个性质，即若结点有两个子女节点，则其后继结点只可能有右子女，因此，可以删除后继结点，之后将后继结点的数据复制到要删除的结点，这样依旧能够保证二叉查找树的性质。当然，若待删除的结点本身只有0个或者1个子女，那就直接删除这个结点。  
综合上述分析，删除操作的过程是先确定要删除的结点，结点本身还是后继；再找到上一步确定的待删除结点的子女；改写子女结点的父节点信息；改写父结点的子女结点信息。			
###12.4 随机构造的二叉查找树
在n个关键字上，随机构造二叉查找树，其期望高度为O(lgn)  
其构造过程如下，对于n个不同的关键字，通过随机顺序，插入到一颗初始为空的树而形成。各输入关键字的n!种排列是等可能的。
##第13章 红黑树
###13.1 红黑树的性质
红黑树是一种二叉查找树，但在每个结点上增加一个存储位表示结点的颜色。  
红黑树：满足如下红黑性质的二叉查找树，称为红黑树，即：根黑，父红子必黑，以每个结点开头的所有路径具有相同的黑结点数目，叶结点（NIL）为黑。  
书中为方便处理红黑树代码的边界条件，使用了哨兵元素nil(T)，其color为黑色，且将红黑树的所有叶节点（NIL）和根结点的父节点都指向哨兵元素nil(T)。  
黑高度：从某个结点（不包括该结点）出发，到达任一叶结点的任意一条路径上，黑色结点的个数称为该结点x的黑高度，用bh(x)表示。  

* 引理13.1   
	一颗n个内结点的红黑树高度至多为2*lg(n+1)  
	首先，归纳法证明结点x为根的树，包含的内结点数目是2^bh(x) - 1。其次，根据红黑树关于黑结点数目的性质，h高度的树，黑高度至少为h/2；两者结合，导出引理。关于归纳法中，证明x高度为0，则x必为nil(T)，这是因为书中已经将叶节点考虑成前文所述的nil(T)，也就是说若x不是nil(T)，其高度至少是1，因为至少有根节点到nil(T)这条边吧。

引理的重要性在于，其保证了在红黑树上进行SEARCH, MINIMUM, MAXIMUM, SUCCESSOR和PREDECESSOR操作的时间为O(lgn)
###13.2 旋转 
* 左旋转  
	可以左旋转的结点x，其右子女y不能是nil(T)，左旋转就是将x与y角色进行互换，根据二叉查找树性质，x只能为y的左子女，而y原先的左子女变成x的右子女（不然的话y就有可能有三个子女）。

		LEFT_ROTATE(T, x)
		y = right(x)
		parent(left(y)) = x
		right(x) = left(y)
		p(y) = p(x)
		if p(y) == nil(T)
			root(T) = y
		else if x == left(p(y))
			left(p(y)) = y
		else
			right(p(y)) = y
		p(x) = y
		left(y) = x 
旋转操作时受到影响的结点为x原来的父结点和右子女节点；y的左子女节点；以及x与y之间的关系调整。
* 右旋转  
	可以右旋转的结点y，其左子女不能是nil(T)，右旋转就是将y与x角色进行互换，根据二叉查找树性质，y只能为x的右子女，而x原先的右子女变成y的左子女（不然的话x就有可能有三个子女）。（参照图13-2）  
**小规律**：从图13-2中，看出，经过左旋或右旋后，除了x与y交换位置外，只有处于中间的子女需要更换父结点，而两侧的子女则不需要调整。
###13.3 插入
插入过程最开始与第12章的插入过程相似，只是到最后需要进行平衡调整，以保证红黑树的性质能满足。

	RB_INSERT(T, z)
	y = nil(T)
	x = root(T)
	while x != nil(T)
		y = x
		if key(x) < key(z)
			x = right(x)
		else
			x = left(x)
	p(z) = y
	if y == nil(T)
		root(T) = z
	else if key(y) < key(z)
		right(y) = z
	else
		left(y) = z
	left(z) = nil(T)
	right(z) = nil(T)
	color(z) = red
	RB_INSERT_FIXUP(T, z)
当插入z（红色结点）时，红黑树的父红子必黑性质可能会被破坏，为保持红黑树性质，需要对结点进行调整。在调整的过程中，可能会对红黑树产生新的破坏，包括两个方面，一是根结点必须为黑色；再一个必须满足父红子必黑的性质。  
以下为调整程序
		
	RB_INSERT_FIXUP(T, z)
	while color(p(z)) == red
		if p(z) == left(p(p(z)))
			y = right(p(p(z)))
			if color(y) == red
				color(y) = black
				color(p(z)) = black
				color(p(p(z))) = red 
				z = p(p(z))
			else 
				if z == right(p(z))
					z = p(z)
					LEFT_ROTATE(T, p(z))
				p(z) = black
				p(p(z)) = red
				RIGHT_ROTATE(T, p(p(z)))
		else
			/* case : p(z) is the right child */
	color(root(T)) = black
对于调整程序中的while循环，列出下列3个循环不变式：  
（1）结点z是红色的  
（2）若p(z)是根，则p(z)是黑色  
（3）若红黑树性质被破坏，只可能是两种情况，根是红色（z是根且z是红色），以及父红子也红（z和p(z)都是红色），且二者只居其一。  
循环不变式（2）有助于确定p(p(z))存在，因为循环条件是p(z)=red，若p(z)是根（黑色），则会退出循环。因此，若进入循环的话,p(z)不是根，则p(p(z))会存在。  

while循环体内，其祖父结点必然是黑色（因为进入循环时p(z)为红色，根据红黑树规则，p(p(z))为黑色），有三种情况（书中只给出了p(z)为左子女的情况，同样的有子女情况也有3种）：   
（1）若z的叔父结点是红色，则将其祖父结点改为红色，而父结点以及叔父结点改为黑色，同时将z = p(p(z))，继续迭代  
（2）若z的叔父结点是黑色，则又细分为两种情况：① z是右子女，首先要对p(z)做左旋转操作（注意代码中的z=p(z)，这样z就变成了p(z)的左子女），以保证z为左子女，之后使用②操作；② z是左子女，只需对祖父结点做右旋转，而这样是满足红黑树性质的（参考图13-6），注意旋转前对颜色进行调整，经过本次旋转后，循环结束。  
**思考**：对于图13-6，case2时为什么不直接右旋，而是先要经过左旋？直接左旋的话会破坏性质5)
###13.4 删除
删除过程最开始与第12章的删除过程相似，只是到最后需要进行平衡调整，以保证红黑树的性质能满足。    	

	RB_DELETE(T, z)
	if left(z) == nil(T) or right(z) == nil(T)
		y = z
	else
		y = TREE_SUCCESSOR(z)
	if left(y) != nil(T)
		x = left(y)
	else
		x = right(y)
	p(x) = p(y)
	if p(y) == nil(T)
		root(T) = x
	else if y == left(p(y))
		left(p(y)) = x
	else
		right(p(y)) = x
	if y != z
		"copy y's satellite data into z"
	if color(y) == black
		RB_DELETE_FIXUP(T, x)
	return y
在RB_DELETE中（**删除的是y，但是调整的是其子女x**），若被删除的结点y是黑色，则会产生三个问题：  
（1）若y原来是根结点，且y的子女颜色为红色，则该子女称为新的根，违反了根必须为黑色的性质；  
（2）若y的子女和y的父亲都为红色，则违反了根红子必黑的性质；  
（3）删除y将使原来包含y的任何路径黑结点个数少了1，这破坏了性质5).  
解决问题（3）的方法是将x额外赋予一个黑色属性，也就是x当做1个或者2个黑结点，如此，就维持了性质5)，但这样性质1)又违背了。  
在while循环中，x的兄弟w不能为nil(T)，因为循环中x是黑色，而x有两重属性（被删除的原父结点的黑色属性赋予了x），若w为nil(T)，则从p(x)出发的路径的黑结点数目会不相等。
为解决这个问题，while循环中分了四种情况：  

* case 1：x的兄弟w是红色的  
    此时w必然有黑色子女，我们对p(x)做一次左旋转（注意，书中的while循环里只考虑了x为左子女的情况；若x为右子女，则p(x)做右旋转），并改变w和p(x)的颜色，此时，x的兄弟为w之前的某个黑色子女，case 1 变成了case2/case3/case4中的一种  
* case 2：x的兄弟w是黑色，而w的两个子女都是黑色  
	若x和w都为黑色，且w的两个子女都为黑色，则将w的黑色和x的额外一层黑色属性移到p(x)，从p(x)开始的黑结点数目是不会发生变化的，因为p(x)此时也是双重颜色，此时p(x)再重复case 1的循环。若case 2是通过case 1进入的，在case 1中，p(x)已经是红色，随后在case 2中执行 x = p(x)，此时color(x)=red（注意：要理解清楚双重属性的含义，x的color依旧用color(x)取得，是红是黑跟附加的一层黑色无关，只是为了性质5)将其标记为附加了一层黑色属性的特殊结点，而这种特殊结点会随着调整而逐渐传递的，比如从x传递到p(x)，此时p(x)就是新的特殊结点），循环条件不满足而退出循环，退出时将x的color赋值为black。若循环由case 2退出，我们很好理解，退出时x（此时x的color肯定是红）的两个子女必然都是黑色，而x需要将附加的黑色算上才能保持性质5)成立，那么，既然x是红色，直接将其color变为黑色便可解决附加的问题。（附加时若x是黑色，则还需要迭代处理，因为x不可能为两重黑色）  
* case 3：x的兄弟w是黑色，而w的左子女为红，右子女为黑    
	对w进行右旋，并交换w和左子女的颜色，此时转为case 4
* case 4：x的兄弟w是黑色，而w的右子女为红色    
  	对p(x)做一次左旋转，由图13-7的（d）所示，在旋转之后，子树alpha和子树beta因为多了一个黑色B结点，此时若将x看成正常的结点，能够满足旋转前后黑结点数目不变的条件，因此此时去掉x的额外黑色，来把它变为单独的黑色，这样x就是正常的结点。可观察，其它子树在旋转前后黑结点数目一致。随后将x赋值为root(T)，结束循环（其实此处通过break结束也行）。
##第14章 数据结构的扩张
本章讨论两种通过扩充红黑树构造的数据结构。
###14.1 动态顺序统计
对第13章中介绍的红黑树，给每个结点x增加了一个size域，用于表示以该结点为根的结点树（含x自身），其中，定义size(nil(T))为0，则有下面等式  

	size(x) = size(left(x)) + size(right(x)) + 1
上述这种树称之为**顺序统计树(order-statistic tree)**，显然其也是一颗红黑树。在该树中，关键字可以不同，在这种情况下，先前排序的定义不再适用，定义排序为按中序遍历树时输出结点的位置。  

从以x为根的子树中检索第i小的关键字：  
	
	OS_SELECT(x, i)
	r = size(left(x)) + 1
	if i == r
		return x
	else if i < r
		return OS_SELECT(left(x), i)
	else
		return OS_SELECT(right(x), i-r)
确定一个元素的秩(x的秩表示按中序遍历排在x前面的元素个数再加1)：  
	
	OS_RANK(T, x)
	r = size(left(x)) + 1
	y = x
	while y != root(T)
		if y = right(p(y))
			r = r + size(p(y)) + 1
		y = p(y)
	return r
对于结点size域的维护：  
在插入和删除操作时，需要对size域进行维护。  
插入操作包含两个阶段，第一个阶段从根开始，沿着树下降，将新结点插入到某个已存在结点；第二个阶段沿树上升，做一些颜色修改和旋转以保持红黑性质。在第一个阶段，每一个经过的结点，其size都要加1（因为size(x) = size(left(x))+sizeof(right(x))+1，而被经过的结点必然是插入点的祖先），插入操作**至多旋转两次**，对旋转涉及到的常数个结点，其size域需要维护。  
删除操作也可以划分为两个阶段，第一个阶段是查找，第二个阶段**至多做三次旋转**。对于第一个阶段，首先搜寻到要删除的结点y，再遍历从y到根的路径，并减小该路径中每个结点的size域，第二个阶段也是涉及到常数个结点的size域需要维护。
###14.2 如何扩张数据结构  
* 数据扩张的四个步骤：
	* 选择基础数据结构
	* 确定要在基础数据结构之上添加的信息
	* 验证可用基础数据结构上的基本修改操作来维护这些新添加的信息
	* 设计新的操作
* 红黑树的扩张定理  
	f是扩张的域，且结点x的域f只与结点x, left(x)和right(x)有关（例如，f(left(x))）。那么，在**插入和删除**操作中，我们可以对所有结点的f值进行维护，而插入和删除的渐进时间依旧是O(lgn)
###14.3 区间树
区间树是一种对动态集合进行维护的红黑树，多了一个区间域int[x]（注：interval简称int），结点x的关键字设置为low(int[x])（即区间的下界），这样，对树进行中序遍历就可按下界的次序列出各区间。**注意，书中将int[x]也当做了基础数据结构。**  
附加信息：max(x)，以x为根的子树中所有区间端点的最大值；  
对附加信息的维护：max(x) = max(high(int[x]), max(left(x)), max(right(x)))  
设计新的操作：

	INTERVAL_SEARCH(T, i)
	x = root(T)
	while x != nil(T) && i does not ovelap int[x]
		if left(x) != nil(T) && max(left(x)) >= low(i)
			x = left(x)
		else
			x = right(x)
要判断两个区间是否相交，肯定是将一个区间的上界与另一个区间的下界进行比较，而区间树中只存储了区间的上界，因此，需要与i的下界进行比较。搜寻代码是用max(left(x))来进行比较，若用max(right(x))，可不可以呢？代码又做何种调整。（<font color='red'>思考：虽然区间树以low(int[x])作为关键字，但hight(int[x])是没有限制的，也有可能max(left(x))大于max(right(x))，但是根据循环不变式中第4行的证明，关于high(i)的隐含假设，代码中就只能使用max(left(x))了</font>）
##第15章 动态规划
>和分治法一样，动态规划（dynamic programming）是通过组合子问题的解而解决整个问题的。分治法是指将问题划分成一些**独立的子问题**，递归地求解各子问题，然后合并子问题的解而得到原问题的解。然而，在**子问题不独立**时（即子问题包含公共的子子问题），若用分治法，则会重复地求解公共的子子问题，动态规划法将每个子子问题的解放入一张表中，从而只需对子子问题求解一次，避免了重复工作。（P192）

动态规划算法的设计可以分为如下4个步骤：  
（1）描述最优解的结构  
（2）递归定义最优解的值  
（3）按自底向上的方式计算最优解的值  
（4）由计算出的结果构造出一个最优解  
###15.1 装配线调度
书中按上述的4个步骤依次求出了最优解，下面只列出关键的部分：  
通过整个工厂的最快路线：

	f = min(f1[n]+x1, f2[n]+x2)
对于上述的f1[n]和f2[n]需要列出递归式（书中公式15.6和公式15.7）,由于公式难以编辑，请直接参考书本。  
对于图15-2中出现的f* 以及l* ，前者f* 表示整个工厂最快路线的最优解（参考公式15.1），而l* 相当于对装配线最后一站的选择，我们可以看到步骤2中从l* 开始逆推。此外，li[j]是对前一个装配站的选择（即对第j-1个装配站，且题中选择只能是1或者2），而li[1]之前是初始值，故li[1]无意义，对于最后一个装配站的选择，反映在前面所述的 l*上。  
若按照公式15.6和公式15.7的递归形式来求解，则复杂度为O(2^n)。  
其实，由递归式分析，f[j]只与f[j-1]有关，因此，将f[j]前面的f[j-1]先保存下来，则要计算f[j]时就不需要重头算起（从f[1]），这样复杂度就是O(n)。也就是说，像f[1]这样的就是公共子子问题，计算一次后保存（本题用的数组），而不需要重复计算。这也是步骤（3）中指明的按自底向上的方式计算出最优解。  
FASTEST_WAY中，实现了上述方案的伪码。
步骤（4）根据计算结果逆推出最优解。
###15.2 矩阵链乘法
矩阵乘法满足结合律，而不同的结合方式会对求积的代价有很大影响，本节探讨了最优结合，也就是书中所述fully parenthesized（即将所有矩阵全加括号，以表明计算优先级）。  
矩阵fully parenthesized的数量，可以用递归式导出，也就是书中的公式15.11，而对于n个矩阵的乘积，其fully parenthesized的数量为n的指数形式，因此，穷举法来求取最优结合（也就是最优fully parenthesized）是不可行的。  
按动态规划算法的4个步骤，解法如下：  
（1）寻找最优子结构  
最优子结构，对于矩阵链(ij)，其中i<=j。若i<j，该问题是非平凡的（nontrivial）。我们假设在k（i<=k<=j）处将矩阵链分开，若分开的两边都是最优fully parenthesized，则对于所选的k值，则矩阵链(ij)也是最优fully parenthesized，那么对于矩阵链(ij)在全局上的最优fully parenthesized，则是比较k的所有可能值之后，选择最优的那一个。  
**此处有两种思想需要体会**：  
① 最优子结构的设计：为什么选择矩阵链(ij)，而不是矩阵链(1j)？要知道，既然是子结构，一定是对原问题进行了划分，那么分析问题时要将原问题能用某个规模来进行表述，比如矩阵链的乘法，是规模为n的多个矩阵的相乘。于是，我们接着考虑划分，首先想到的是，对这n个矩阵分割为前后两个部分，前半部分(1...k)和后半部分(k+1...n)；其次，划分之后要能够提炼出通项（感觉涉及递归用法的都需要寻找通项，包括前面的分治法在计算复杂度时也是，因为分治法也要考虑子问题划分），若要兼容前后部分，那么通项的矩阵链只能是矩阵链(ij)而不是矩阵链(1j)。  
②关于选择，对原问题进行划分，可能有多种分法，而最优子结构是其中的一种（若有多种情况都是最优，其实也只需要考虑一种），这里主要想表达的意思是，划分之后的子结构是否为最优，还需要后续的比较验证，因此思考问题时，若遇到了划分之后无法证明该子结构是否最优的这种情况，还需要继续向下思考，给问题加上额外条件（如多种划分导致的多种选择），比如矩阵链乘法多种划分后的选择问题，这样能更有效的解决问题。  
（2）寻找通项  
在划分好子结构后，针对所假设的划分，提炼出通项。**注意**，此处的通项不一定就是直接对原问题进行通项求解，比如，划分的子结构是矩阵链(ij)，此处的通项只考虑了ij，而不是从1开始的具体通项。  
（3）计算最优代价  
注意，此处是代价，而不是最优解。代价相当于选择这种解法时的成本；而最优解则是告诉你具体怎么操作。  
原问题划分为子问题求解时，在各递归步骤中，其有效的划分(i,j)需要满足1<=i<=j<=n，因此，共有O(n^2)种子问题。

	MATRIX_CHAIN_ORDER(p)
	n = length(p) - 1
	for i = 1 to n
		m[i, i] = 0
	for l = 2 to n
		for i = 1 to n-l+1
			j = i+l-1
			m[i, j] = +inf
			for k = i to j-1
				q = m[i,k] + m[k+1,j] + p(i-1)*p(k)*p(j)
				if q < m[i,j]
					m[i,j] = q
					s[i,j] = k
上述代码中，p={p(0),p(1),...,p(n)}，其中Ai的维数为p(i-1)*p(i)。  
（4）构造最优解  
根据s[i,j]的记录，递归的求取。首先s[1,n]可以确定k，对于前半部分s[1,k]和后半部分s[k+1,n]又可以确定k1和k2，逐层递归下去。
###15.3 动态规划的基础  
适合采用动态规划方法的最优化问题中的两个要素：最优子结构和重叠子问题。  

* 最优子结构  
	如果一个问题的最优解中包含了子问题的最优解，则该问题具有最优子结构。在动态规划中，我们利用子问题的最优解来构造问题的一个最优解，因此，在考虑的子问题中，必须确保包含了一个最优解中的那些子问题。  
	* 寻找最优子结构的一般模式（common pattern）：  
	（1）问题的一个解可以是做一个选择。（该解不一定是最优的，通常是需要在多个选择对应的多个解中寻找最优）  
	（2）可以假设某个选择是可以获得最优解，可以仅做假设，无需证明。（因为步骤（4）会证明）  
	（3）在步骤（2）中的选择下，要确定对应的子问题，并描述得到的子问题空间。  
	（4）利用剪贴技术（cut-and-paste），来证明在问题的一个最优解中，使用的子问题的解本身也是最优的。一般采用反证法，假设当前的子问题不是最优，利用剪贴技术，将不是最优的剪去，而将最优的贴过来，这样使得原问题也不是最优了，那么便导出矛盾。
	* 描述子问题空间的经验规则  
		尽量保持这个空间简单，然后在需要时扩充它。
	* 最优子结构在问题域中以两种方式变化  
		（1）有多少个子问题被使用在原问题的一个最优解中  
		（2）在决定一个最优解中使用哪些子问题时，选择的种数（也即在多个可能的选择中，可以获得子问题的一个最优解，当然，有些选择不一定能获得最优解，但至少有一个可以获得最优解的选择）。  
		综合（1）（2），可以得出，一个动态规划算法的运行时间依赖于两个因素的乘积，子问题的总个数和每一个子问题中有多少种选择。
	* 自底向上方式利用最优子结构  
	* 一些细微之处  
		书中举了“无权最短路径”和“无权最长简单路径”两个例子，讲述了关于子问题的独立性。我们知道，正是因为子问题之间有重叠的子子问题，而我们只对重叠的子子问题求解一次，从而降低复杂度，那么子问题到底是相关的还是独立的？应该这么说，对于原问题的求解，若对于某一个可能解，其分解为了两个子问题，则这两个子问题必须独立；若还有一个可能解，也分解为了两个子问题，这两个子问题与前面那个可能解的两个子问题是否独立，是无关紧要的，但同一个解的两个子问题必须要独立。那么，如何看待重叠的子子问题呢？这就涉及到两个方面，第一，并列的不要求相互独立的子问题可能有重叠，这个就如前面两个可能解中的子问题之间，相当于在递归的同一层；第二，不同递归层次的子问题具有重叠子问题，这个可以参考装配线问题。

* 重叠子问题  
	子问题的空间要很小，用来求解原问题的递归算法是在反复求解同样的（重叠）子问题，而不是总是产生新的子问题。对于分治法，每一次递归都是产生新的子问题。
* 重新构造一个最优解  
	需要额外的存储空间，以便存储过程中的最优信息。  
* 做备忘录 Memoization  
	动态规划的一种变形，采用自顶向下的方式来求解问题。也是维护一个记录了子问题解的表，但有关填表动作的控制结构更像递归算法。  
    >在实际应用中，如果所有的子问题都至少要被计算一次，则一个自底向上的动态规划算法通常要比一个自顶向下的做备忘录算法好出一个常数因子。如果子问题空间中的某些子问题根本没有必要求解，做备忘录方法有着只解那些肯定要求解的子问题的优点。（P208）
###15.4 最长公共子序列  
主要是寻找出最优子结构，寻找时要结合到选择，也就是可以赋予的额外条件，比如，本节中的x和y序列最后一个元素的比较，不同的比较结果可以有不同的递归式。  
关于构造最优解，如何做记录以及如何构造出来，本节的方法也是很有借鉴意义的。  
**改进代码**：  
可以把为构造最优解而做记录所占的空间b[i,j]拿掉，直接用c[i,j]之间的关系，可以构造出递归解。
###15.5 最优二叉查找树  
最优二叉查找树：给定一个由n个互异的关键字组成的序列K={k1,k2,...,kn}，且关键字有序（k1<k2<...<kn），关键字ki被搜索到的概率为pi，另外，插入n+1个虚拟关键字d0,d1,...,dn，且ki<di<k(i+1)【注，此处为下标】，搜索到di的概率为qi。虚拟关键字是不存在的，只是搜索ki时若找不到，必然落到di中（注意，di肯定为n+1个，d0<k0<d1<k1....，从这个不等式就看出来了），再结合附录B.5-3的习题，di必定全部为叶节点。  
对于给定的一组概率，我们的目标是构造一个期望搜索代价最小的二叉查找树，把这种树称作最优二叉查找树。  
在树T内进行一次搜索的代价为（公式15.16）：  
搜索每个结点时，搜索到该结点时已经进行了depth+1次比较操作（包括对结点本身的比较），若该结点的被搜索概率为pi，则搜索到该节点的期望次数就是(depth+1)*pi  
动态规划方法，步骤：  
（1）一棵最优二叉查找树的结构  
任意一棵子树必定包含连续关键字ki, k(i+1),...,kj，其叶子结点为d(i-1),di,...dj。
陈述最优子结构：如果一棵最优二叉查找树T有一棵包含关键字ki, k(i+1),...,kj的子树T'，那么这棵子树也是最优的。  
如何去找到一棵最优子树，对于上述的T'，假设kr（i<=r<=j）是根，对于所有的候选根kr，确定其对应的左子树（ki,k(i+1),...,k(r-1)）和右子树(k(r+1),...,kj)也是最优二叉查找树，  
（2）一个递归解  
e[i,j]为搜索一棵最优二叉查找树的期望代价，最终需要计算出e[1,n]，其中e[i,i-1] = q(i-1)。
因此，对于以kr为根的最优子树，其最优左子树的搜索期望代价为e[i,r-1]，最优右子树的搜索期望代价为e[r+1,j]，【注意，e[i,r-1]和e[r+1,j]是作为独立的树的时候的期望搜索次数，当树作为某个结点的子树时，该树被搜索到的次数还需要加上该结点一次，也就是公式15.16中，是depth+2，这样，当树作为某个结点的子树时，其搜索的期望次数为e[i,r-1]+w(i,r-1)，其中，后者为depth+2相对于depth+1多出的项，也就是公式15.17】，于是，我们便得到了公式15.18.  
（3）计算期望搜索代价  
对于公式15.18中的递归式，w(i,j)的递归式也是需要考虑的。  
##第16章 贪心算法 
贪心算法是使所做的选择看起来是当前最佳的，期望通过所做的局部最优选择来产生一个**全局最优解**。  
动态规划是贪心算法的基础。贪心策略通常是自顶向下，逐步缩小子问题，到最后求出结果。
###16.1 活动选择问题   
活动按结束时间的单调递增顺序进行了排序。  

* 按动态规划解法步骤来使用贪心算法  
	（1）活动选择问题的最优子结构  
		与第15章一样，本问题用到的技巧，感觉就是首先对活动进行了排序（若已知条件并没有排序，解决问题时要有这方面的意识），这样方便划分子问题。同时，为方便表述，首尾的边界可能需要添加特殊值，这又有点类似哨兵元素的思想。
		进行前述处理之后，文中首先确定了需要从Sij中选择子问题，其中，0<=i<j<=n+1（注意i和j需要满足的条件），不满足这些条件的i和j所对应的Sij为空集。**注意：此处的思想需要学习**，即将原问题进行了调整（排序，当然，本体给出的条件就是直接排过序的），以方便最优子结构的描述，且此处的Sij下界为i活动的结束，上界为j活动的开始，添加合适的哨兵元素后，原问题S便可描述为S(0,n+1)，括号内为下标，而且这样描述也便于划分子结构。  
	（2）一个递归解  
		c[i,j]为Sij中最大兼容子集中的活动数，因此，便可以定义出递归式。  
	（3）将动态规划解转换为贪心解  
		当然，根据（2）已经可以采用动态规划求解了。  
		定理16.1可用于最优解对应的两个子问题的选择，运用该定理，发现只需求解其中一个（因为另一个为空）。这里需要注意，如何使得最优解对应的两个子问题中有一个为空，这就是贪心算法所要面对的难题。正如本节前面所述（P223），贪心算法只需要考虑一个选择（亦即，贪心的选择），在做贪心选择时，子问题之一必是空的，因此只留下一个非空子问题。  
* 递归贪心算法和迭代贪心算法  
	首先要注意，活动都是按照结束时间已被排序的，故按序遍历只要开始时间在Sij之内，第一个满足该条件的活动即为满足定理16.1的活动。
###16.2 贪心策略的基本内容  
* 贪心算法的步骤
	（1）决定问题的最优子结构  
	（2）设计出一个递归解  
	（3）证明在递归的任一阶段，最优选择之一总是贪心选择。那么，做贪心选择总是安全的。  
	（4）证明通过做贪心选择，所有子问题（除一个以外）都为空。  
	（5）设计出一个实现贪心策略的递归算法  
	（6）将递归算法转换为迭代算法  
* 贪心算法的一般解法：  
	（1）将优化问题适当进行转换，比如16.1中的例子，以方便寻找最优子结构  
	（2）证明原问题总有一个最优解是做贪心选择得到的，从而说明贪心选择的安全。  
	（3）在做贪心选择后，剩余的子问题具有这样的性质：若子问题的最优解和我们所做的贪心选择联合起来，可以得到原问题的一个最优解。  
<font color='red'>关于背包问题，还需做题</font>
###16.3 Huffman编码	
可变长度编码与固定长度编码。  
前缀编码（prefix code）： 没有一个编码是另一个编码的前缀。只要识别出第一个码，其余编码重复该过程即可。  
前缀编码的解码：通过二叉树进行（注意，非二叉查找树，因为无需排序，结点也不是关键字）。  
文件的一种最优编码总是由一棵满二叉树来表示，即非叶节点都有两个子结点。编码一个文件所需的位数见公式16.5  
Huffman过程：从n个字符（对象）中，先合并概率最小的两个对象，形成一个新的结点对象（概率为二者之和），而两个字符分别为左右子树，假设分别编码为0和1（当然是1和0也可以）；在随后的n-1个结点中，也是类似前面步骤，找出概率最小的两个对象进行合并；最终剩下一个结点为树的根。  

* Huffman算法的证明  
	（1）引理16.2，最低频度的两个字符x和y，当其编码长度相同但最后一位不同时，那么存在一种C的最优前缀编码。对于该证明的分析，若x和y是最大深度的兄弟叶结点，那么肯定是编码长度相同但最后一位不同，因此，看能否导出x和y是最深兄弟叶结点的最优前缀编码树。那么，我们先假定有一颗最优前缀编码树，但x和y不一定是最深兄弟叶结点，而最深兄弟叶结点是a和b（注意，最优前缀编码树是满二叉树），那么，可以做一般假设f(x)<=f(a)，f(y)<=f(a)，将a和x交换位置，得到新树T'，发现B(T)<=B(T')，再在T'基础上交换b和y，发现B(T')<=B(T'')，而T已假设为最优，那么导出T''为最优，同时x和y为最深兄弟叶结点，进而得证。  
	（2）引理16.3，该引理证明时，主要是清楚题设T'已经为最优前缀编码的任意一棵树。有点像从最优子问题来导出最优原问题的感觉。

###16.4 贪心算法的理论基础  
* 拟阵  
	<font color='red'>待看，主要讲了拟阵的性质和用于贪心算法的情形</font>
###16.5 一个任务调度问题  
<font color='red'>用拟阵来解决问题，待看</font>
##第17章 平摊分析  
平摊分析（amortized analysis）： 执行一系列数据结构操作所需的时间是通过对执行的所有操作求平均而得出的。平摊分析不涉及到概率。  
###17.1 聚焦分析  aggregate analysis  
 在聚焦分析中，要证明对于所有n，由n个操作(每次的操作类型一般不同)所构成的序列的总时间在最坏情况下为T(n)，因此，其平摊代价（amortized cost）为T(n)/n。  
本节以栈操作和二进制计数器递增1这两个例子，进行了说明。  

* 栈操作  
	以PUSH, POP和MULTIPOP三种类型的操作为例，对于任意n值，三种操作的平摊代价为O(1)。因为POP和MULTIPOP弹出的元素肯定是PUSH进去的，即使MULTIPOP(S, m)的时间是O(m)【注意，当栈内元素个数小于m时，则时间是元素的具体数量】，而PUSH是O(1)，即使n次操作全是PUSH，那么POP和MULTIPOP的代价为T(n)，故平摊代价是O(1)。**要学习这种时间分析的思想，并注意是最坏情况下的非概率分析。**  
* 二进制计数器递增1  
	将每位A[i]的翻转的次数进行计算，所有位的总次数就是加1的代价，进行n次增1的代价为O(n)，平摊就为O(1)。这与6.3中建堆的分析类似。
* 小结  
	我们从聚焦分析的定义中，看到，n个操作最坏的情况下总时间为T(n)。从所举的两个例子来看，栈操作中，有两种相反的操作，犹如输入和输出，只有一种操作有了，另外一种操作才能进行，而有的那一种操作若是O(n)，则相对的那种操作无论怎样组织（如POP和MULTIPOP），最多也是O(n)；二进制计数器递增1，则是n种可能的操作类型下，对其进行平均，则每种操作为O(1)。
###17.2 记账方法  
一个操作的平摊代价定义为

	平摊代价 = 实际代价 + 存款
本章所列出的三种方法都是为了估算n个操作的代价上限（最坏情况），因此，n个操作的平摊代价之和要大于其实际代价之和，即总存款要求非负。对于单个操作，其平摊代价可以低于实际代价，只要n个操作整体上满足前述条件即可。 

* 栈操作  
	例子中PUSH的平摊代价为2，而POP和MULTIPOP都为0，每PUSH一次，花费1元，那么还有存款1元，也就是每个盘子有1元存款，在弹出时，直接使用存款便可（存款也够支付实际费用），而无需另外花费（此例中POP的平摊代价为0），这样总代价就为PUSH的平摊代价之和，若n次PUSH，则为O(2n)。**对于这类问题，对源头设置两倍实际代价的平摊代价即可**  
* 二进制计数器递增1  
	将二进制的某一位设置为1的平摊代价为2，而设置为0的平摊代价为0，分析同“栈操作”例子。从代码中可以看出，每次加1时，至多有一位被设置为1，而每个被设置为1的位再被清零时，用存款即可。**注意：**值为1的位肯定有存款，对其清零使用存款即可（即无需额外代价），在对某一位加1之前，比这一位低的位都是进行清零操作（二进制加1的特性），而每一次最多只会设置一位为1（平摊代价为2），因此，每次加1的平摊代价为2，n次就为O(2n)。  
###17.3 势能方法  
记账方法的存款是与个别对象发生联系（如17.2节加法中的位），而势能方法是与整个数据结构发生联系。  
每个数据结构映射为一个实数，即与**数据结构**联系的势。   
通过公式17.2和17.3可以得出势函数的关系，以及对于势函数的要求。  

* 栈操作  
	势函数定义为，栈的元素个数，可见该定义是满足公式17.3的。由此可以到处PUSH，POP和MULTIPOP的平摊代价。
* 二进制计数器递增1   
	势函数定义为，计数器中1的个数，注意分析时引进的变量b和t。
###17.4 动态表  
* 表扩张  
	在书中的TABLE_INSERT的程序中，其实可以只计算INSERT的代价（那些分配空间、释放空间等代价可以算到INSERT上），分析时，要注意利用6.3中建堆那种细分的思想。  
	**记账方法的分析**，插入一次平摊代价算3，为什么？若表的大小为m，则在超过m/2后，插入时，自身所用代价为1，存储1用于表扩张时自身的移动，另送给前面m/2个元素中的某一个1，以便其用于扩张时的移动。（当然前m/2的代价是1）    
	**势能方法的分析**，势能函数选为2 * num - size（num为第i次操作后元素个数，size为表格大小）	  
	**图17-3的解释**，num线性增长，这个不用多说。size为什么不是阶跃函数呢？举个例子，i=16时size为16，而i=17时，size才为32，显然i从16到17不是阶跃的，即为图中的虚线。那么phi的曲线又做何解释，依旧以i=16为例，此时phi增长到16，在i=17后，此时size为32，这样一减，phi为4，其实每次扩张时phi都是为4，自己可以验证。
* 表扩张和收缩  
	可能遇到的问题，插入使得元素数量超过m后，m->2m，删除使得元素数量小于m时，则2m->m，那么，又进行插入操作，则又m->2m。这可能导致O(n^2)的代价。  
	解决方案：当删除一项而引起表不足1/4时，才将表收缩。  
	此处用的势函数根据装载因子alpha进行了分段。  
	需要考虑插入和删除两种情况，而每种情况又细分多个子类。  
##第18章 B树  
>在一个典型的B树应用中，要处理的数据量很大，因此无法一次都装入主存。B树算法将所需的页选择出来复制到主存中去，而后将修改过的页面再写回到磁盘上去。因为在任何时刻，B树算法在主存中都只需要一定量的页数，故主存的大小并不限制可被处理的B树的大小。(P264)  
>在B树中，一个结点的大小通常相当于一个完整的磁盘页。因此，一个B树结点可以拥有的子女数就由磁盘页的大小所决定。（P265）

运行时间的两个主要部分：  
（1）磁盘存取的次数  
（2）CPU（计算）时间，也就是算法复杂度，如堆排序为O(nlogn)
###18.1 B树的定义  
* B树的性质（定义）  
	n[x]，存储在结点x中的关键字个数，且关键字以升序存放，关键字将各子女进行隔离，也就是有n[x]+1个子女  
	leaf[x]，表明结点是否为叶结点  
	若是内结点，则还包括指向其n[x]+1个子女的指针域（每个子女对应一个指针）  
	每个叶节点具有相同的深度  
	每个结点包含的关键字的上界和下界，表示为t的函数，其中t至少为2，t称为**最小度数**  
	每个非根结点必须至少包含t-1个关键字，那么就至少有t个子女。若树非空，根结点至少包含1个关键字，**也就是说根节点的最小关键字数没有限制为是t-1，但最大关键字数则限制为2t-1**。  
	每个结点的关键字至多为2t-1，那么就至多有2t个子女，若一个结点满了，则其有2t-1个关键字。  
* B树的高度  
	n个结点组成的B树，可能的高度最大值，便是按图18-4的组法。证明过程很简单。   

本节最后简略的比较了B树与红黑树，B树的高度（访问次数）比红黑树少log(t)的常数因子，虽然都是log(n)，但常数因子也会产生很大的影响。 
	  	
###18.2 对B树的基本操作  
* 两个约定  
	根节点始终在主存中，无需DISK_READ，当根结点改变时，需要做一次DISK_WRITE  
	任何被当做参数的结点被传递之前，需要先做一次DISK_READ
* 搜索B树  
	CPU时间 tlogt(n)  
	与二叉查找树的搜索方式不同的是，由于每个结点有n[x]个关键字，因此，也要定位到第一个不小于待查找值k的位置，之后比较是否找到，若没有，则根据情况返回NIL或者递归搜索。
* 创建一颗空的B树  
	O(1)次的磁盘操作和O(1)的CPU时间，注意需要写磁盘操作  
* B树中结点分裂  
	x的第i个y孩子若满了，则将y分裂成y和z，各含t-1个关键字，中间的关键字被提到x，x相当于增加了一个孩子。  
	分裂时，涉及到三个结点，对于新增的结点z，其要从y处拷贝key值，leaf属性，以及指向子女的c值；对于结点y，关键字数目要调整为t-1；对于结点x，由于其多了一个孩子z，故用c(i+1)指向z，且原来的c从i+1开始下标加1（即原来的c(i+1)变为现在的c(i+2)，后移一个），对于关键字，显然也需要插入一个关键字，原来y有2t-1个关键字，现在y和z各有t-1个关键字，中间的那个关键字，被提做x的第i个关键字，而原先的第i个及之后的往后移一个下表。
* 向B树插入关键字  
	在前面学到的二叉查找树中插入结点，是创建一个新的叶结点实现的。在B树中，叶结点需要保持一样的高度，因此，插入结点是将结点插入到已存在的叶结点中。但是当已存在的叶结点满了之后，需要进行结点分裂。  
	在实际插入时，从根结点开始下降到叶子，若沿途遇到每个满结点，就将其分裂，这样就能保证当要分裂一个满结点y时，其父结点不是满的。  
* 对B树用单程下行遍历树方式插入关键字  
	
		B_TREE_INSERT(T, k)
		r = root(T)
		if n(r) == 2t-1
			s = ALLOCATE_NODE()
			root(T) = s
			n(s) = 0             /* 因为split操作时会对其加1，故此处设置为0 */
			leaf(s) = FALSE
			c1(s) = r
			B_TREE_SPLIT_CHILD(s, 1, r)
			B_TREE_INSERT_NOTFULL(s, k)
		else
			B_TREE_INSERT_NOTFULL(r,k)
B树的高度增加不是在底部，而是在顶部（对根进行分裂）。	  
B\_TREE\_INSERT\_NOTFULL也是向下递归进行插入的，若搜寻到叶结点，则直接进行插入（注意，插入位置肯定是不满的）；若不是叶结点，则还需要进行是否满了的判断，若满了，需要先进行B\_TREE\_SPLIT\_CHILD操作，之后进行递归；若不满，直接递归。**注意，DISK_READ的操作**  
###18.3 从B树中删除关键字  
删除结点x  
<font color='red'>待完成</font>
##第19章 二项堆 binomial heap  
###19.1 二项树和二项堆  
* 二项树定义  
	二项树B0只包含一个结点，二项树Bk由两棵二项树B(k-1)连接而成，其中一棵树的根是另一棵树的根的最左孩子。  
* 二项树性质  
	共有2^k个结点  
	树的高度为k  
	根的度数为k，若从左往右将子女编号为k-1,...,0，则子女i为子树Bi的根  
	在深度i处，有C(k,i)个结点【**此性质可用于解释二项树名称的来源**】  
* 二项堆的定义  
	二项堆H是由多个二项树组成，这些二项树满足以下条件：  
	（1）每个二项树中，结点关键字大于等于其父结点的关键字。（本文讨论的是最小二项堆）  
	（2）对于任意非负整数k，在H中，至多只有一棵二项树的根具有度数k  
* 二项堆的表示  
	二项堆中的每颗二项树，都按照左孩子，右兄弟的表示方式存储，即对结点x，有指向父结点的域p(x)、指向左孩子结点的域child(x)和指向右兄弟结点的域sibling(x)。  
	根表：一个二项堆中的各二项树的根被组织成一个链表，称之为根表，**在遍历根表时，各根结点的度数是严格递增的**。    
	
	各二项树的根结点，其sibling(x)指向根表的下一个根。  
	head[H]指向根表的第一个根  
###19.2 对二项堆的操作  
* 创建一个新堆  
	
		head(H) = NIL
* 寻找最小关键字  
	
		BIONOMIAL_HEAP_MINIMUM(H)
		y = NIL
		min = +inf
		x = head(H)
		while x != NIL
			if key(x) < min
				min = key(x)
				y = x
			x = sibling(x)
		return y
* 合并两个二项堆  
	二项堆H1和H2，首先，将两者的根表进行合并，按根结点的度数进行递增排序，对于每一个度数值，最多对应有两个根（因为二项堆的性质决定：每个堆中，度数为k的二项树最多只能有一个）。
				
##附录

###B.5
* 几个定义  
**自由树**：一个**连通且无回路**的无向图。  
**森林**：非连通且无回路的无向图。  
**有根树**：一颗自由树，但其有一个与其它点不同的结点，这个特殊的顶点称为树的根。  
**有序树**：子女节点有序的有根树。  
**二叉树**：是定义在有限集上的结构。要么不包括任何结点（称为空树或零树，用NIL表示），要么由三个不相交的结点集构成：根结点，一个称为左子树的二叉树和一个称为右子树的二叉树。  
**二叉树与有序树的关系**：二叉树更严格一点，例如，某结点其只有左子女或者右子女，这两种情况对应两种二叉树，而对有序树来说，这两种情况是不做区分的。  
**位置树**：结点中的子女用不同的正整数标识，若没有结点被标识成整数i，则该结点的第i个子女缺失。  
**K叉树**：每个结点的标识超过k的子女均缺失的位置树。  
**完全k叉树**：所有叶结点都有相同深度，并且所有内部结点度都为k  
**度**：结点x的子女数目称为度。  
**外部结点（叶结点）**：没有子女的结点。  
**内部结点**：非叶结点称为内部结点。  
**深度**：从根结点r到结点x路径的长度称为x的深度（即边数）。  
**高度**：从结点向下至某个叶结点最长简单路径中边的条数。  
**树的高度**：树中结点的最大深度，也是根结点的高度。  
深度和高度不要混淆，两者都是用边数来衡量，且可以理解二者有互补之意，即一个结点在这两个方面不可能同时都很大（好处不能都占，是吧？）。

